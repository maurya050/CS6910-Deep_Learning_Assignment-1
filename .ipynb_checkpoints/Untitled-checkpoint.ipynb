{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead6691e",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfb9516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LossFunction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23636\\510206151.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Add layer sizes for the hidden layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"beta1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.09\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"beta2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"epsilon\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"weight_decay\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss_fun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLossFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cross_entropy\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Loss Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Optimizer : adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LossFunction' is not defined"
     ]
    }
   ],
   "source": [
    "#Add layer sizes for the hidden layers\n",
    "parameters = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
    "loss_fun = LossFunction(\"cross_entropy\") # Loss Function\n",
    "layers = [32, 64, 128]\n",
    "optimizer = Optimizer(\"adam\") # Optimizer : adam\n",
    "optimizer.set_initial_parameters(parameters)\n",
    "activation = ActivationFunction(\"relu\") #activation(hidden layers)\n",
    "output_activation = ActivationFunction(\"softmax\") #activation(output layer)\n",
    "\n",
    "model = FFNN(optimizer, layers, loss_fun, activation, output_activation, 64, 15, initialization = \"Xavier-Normal\")\n",
    "train_loss, val_loss, train_accuracy, val_accuracy = model.fit(x_train, y_OH_train, x_test, y_OH_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eebe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for Test Data(Validation)\n",
    "print(\"Validation Loss:\",val_loss ,\" Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e5f3a",
   "metadata": {},
   "source": [
    "# Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "wandb.init(project=\"CS6910_Assignment_1\", name=\"Question:7\")\n",
    "y_p = np.argmax(y_preds,1)\n",
    "y_t = np.argmax(y_OH_test,1)\n",
    "conf_matrix = metrics.confusion_matrix(y_t , y_p)\n",
    "df_conf_matrix = pd.DataFrame(conf_matrix)\n",
    "plt.figure(figsize=(8, 8))\n",
    "my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"yellow\",\"orange\"])\n",
    "ax = sns.heatmap(df_conf_matrix, annot=True,  cmap= my_cmap, fmt='d',linewidths=3, linecolor='white')\n",
    "ax.set_xticklabels(class_label,rotation=90)\n",
    "ax.set_yticklabels(class_label,rotation=0)\n",
    "plt.title('Confusion Matrix', fontsize=8)\n",
    "plt.ylabel(\"Predicted Class\")\n",
    "plt.xlabel(\"True Class\")  \n",
    "wandb.log({\"Confusion_Matrix\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f95c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a78c7",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parameters Selection for Different Optimization Algorithm '''\n",
    "# parameters_nadam = {\"learning_rate\":0.0006, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.001} #nadam\n",
    "# parameters_sgd = {\"learning_rate\":0.001, \"weight_decay\":0.5} #sgd\n",
    "# parameters_momentum = {\"learning_rate\":0.0001, \"gamma\":0.6, \"weight_decay\":0} #momentum\n",
    "# parameters_nag = {\"learning_rate\":0.001, \"gamma\":0.009} #nag\n",
    "# parameters_rmsprop = {\"learning_rate\":0.01, \"gamma\":0.009, \"epsilon\":1e-8, \"weight_decay\":0.5} #rmsprop\n",
    "parameters_adam = {\"learning_rate\":0.0006, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.001} #adam\n",
    "\n",
    "'''For Selecting different Optimization Function'''\n",
    "# \"sgd\" : gradient_descent, \"momentum\" : momentum_gd, \"nag\": nag , \"rmsprop\":  RMSProp, \"adam\": Adam \"nadam\": Nadam\n",
    "\n",
    "optimizer = Optimizer(\"adam\")\n",
    "optimizer.set_initial_parameters(parameters_adam)\n",
    "\n",
    "\n",
    "#  \"cross_entropy\" : Cross Entropy Loss Function,  \"squared_loss\" : Squared Error Loss Function\n",
    "\n",
    "loss_fun_se = LossFunction(\"squared_loss\")\n",
    "train_se_losses = []\n",
    "val_se_losses = []\n",
    "train_se_accuracy = []\n",
    "val_se_accuracy = []\n",
    "\n",
    "\n",
    "\n",
    "loss_fun_cross = LossFunction(\"cross_entropy\")\n",
    "train_ce_losses = []\n",
    "val_ce_losses = []\n",
    "train_ce_accuracy = []\n",
    "val_ce_accuracy = []\n",
    "\n",
    "#Select activation-function(hidden layers) pass below respective string to select any Activation Fuction Eg:\"tanh\" in ActivationFunction\n",
    "# \"sigmoid\": SigmoidFunction, \"softmax\": SoftmaxFunction, \"tanh\": TanhFunction, \"relu\":ReLUFunction\n",
    "act_fun_hidden = ActivationFunction(\"tanh\")\n",
    "\n",
    "#Select activation - function for output layer\n",
    "act_fun_output = ActivationFunction(\"softmax\")\n",
    "\n",
    "#Add layer sizes for the hidden layers\n",
    "layers = [32, 64, 128]\n",
    "\n",
    "print(\"SQUARED_ERROR\")\n",
    "model = FFNN(optimizer, layers, loss_fun_se, act_fun_hidden, act_fun_output, 64, 10, \"Xavier\", 0, train_se_losses, train_se_accuracy, val_se_losses, val_se_accuracy)\n",
    "se_train_loss, se_val_loss, se_train_accuracy, se_val_accuracy = model.fit(x_train, y_OH_train, x_test, y_OH_test)\n",
    "\n",
    "print(\"***********************************************************************************************************\")\n",
    "\n",
    "print(\"CROSS_ENTROPY\")\n",
    "model = FFNN(optimizer, layers, loss_fun_se, act_fun_hidden, act_fun_output, 64, 10, \"Xavier\", 0, train_ce_losses, train_ce_accuracy, val_ce_losses, val_ce_accuracy)\n",
    "ce_train_loss, ce_val_loss, ce_train_accuracy, ce_val_accuracy = model.fit(x_train, y_OH_train, x_test, y_OH_test)\n",
    "ln = len(val_ce_accuracy)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"CS6910_Assignment_1\", name= \"Question:8\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "sizelst = list(range(1, ln))\n",
    "plt.plot(sizelst, val_se_losses, 'r', label ='Squared_Loss') \n",
    "plt.plot(sizelst, val_ce_losses, 'g', label ='Cross_Entropy')\n",
    "plt.title('Validation Loss Comparison: Squared Loss v/s Crossentropy Loss', fontsize=20)\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\") \n",
    "plt.legend()\n",
    "wandb.log({\"Validation Loss per epoch for Squared v/s CrossEntropy Loss \": wandb.Image(plt)})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = mnist.load_data()\n",
    "enc = OneHotEncoder()\n",
    "#Normalize the data\n",
    "x_mnist_train = x_mnist_train/255.0\n",
    "x_mnist_test = x_mnist_test/255.0\n",
    "\n",
    "y_OH_test_mnist = enc.fit_transform(np.expand_dims(y_mnist_test, 1)).toarray()\n",
    "\n",
    "#Splitting to get 0.1 parts of data as validation set\n",
    "x_mnist_train, x_mnist_val, y_mnist_train, y_mnist_val = train_test_split(x_mnist_train, y_mnist_train, test_size=0.1, random_state=137)\n",
    "y_OH_train_mnist = enc.fit_transform(np.expand_dims(y_mnist_train, 1)).toarray()\n",
    "y_OH_val_mnist = enc.fit_transform(np.expand_dims(y_mnist_val, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Selection for Different Optimization Algorithm\n",
    "parameters_sgd = {\"learning_rate\":0.001, \"weight_decay\":0.5} #sgd\n",
    "parameters_momentum = {\"learning_rate\":0.0001, \"gamma\":0.6, \"weight_decay\":0} #momentum\n",
    "parameters_nag = {\"learning_rate\":0.001, \"gamma\":0.009} #nag\n",
    "parameters_rmsprop = {\"learning_rate\":0.01, \"gamma\":0.009, \"epsilon\":1e-8, \"weight_decay\":0.5} #rmsprop\n",
    "parameters_adam = {\"learning_rate\":0.0006, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.001} #adam\n",
    "parameters_nadam = {\"learning_rate\":0.0006, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.001} #nadam\n",
    "\n",
    "\n",
    "# \"sgd\" : gradient_descent, \"momentum\" : momentum_gd, \"nag\": nag , \"rmsprop\":  RMSProp, \"adam\": Adam \"nadam\": Nadam\n",
    "\n",
    "optimizer = Optimizer(\"adam\")\n",
    "optimizer.set_initial_parameters(parameters_adam)\n",
    "\n",
    "\n",
    "#  \"cross_entropy\" : Cross Entropy Loss Function,  \"squared_loss\" : Squared Error Loss Function\n",
    "loss_fun = LossFunction(\"cross_entropy\")\n",
    "\n",
    "#Select activation-function(hidden layers) pass below respective string to select any Activation Fuction Eg:\"tanh\" in ActivationFunction\n",
    "# \"sigmoid\": SigmoidFunction, \"softmax\": SoftmaxFunction, \"tanh\": TanhFunction, \"relu\":ReLUFunction\n",
    "act_fun_hidden = ActivationFunction(\"relu\")\n",
    "\n",
    "#Select activation - function for output layer\n",
    "act_fun_output = ActivationFunction(\"softmax\")\n",
    "\n",
    "#Add layer sizes for the hidden layers\n",
    "layers = [32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664605e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = FFNN(optimizer, layers,  loss_fun , act_fun_hidden, act_fun_output, 64, 10, initialization = \"Xavier\")\n",
    "model_1.fit(x_mnist_train, y_OH_train_mnist, x_mnist_val, y_OH_val_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(\"nadam\")\n",
    "optimizer.set_initial_parameters(parameters_nadam)\n",
    "\n",
    "\n",
    "#  \"cross_entropy\" : Cross Entropy Loss Function,  \"squared_loss\" : Squared Error Loss Function\n",
    "loss_fun = LossFunction(\"cross_entropy\")\n",
    "\n",
    "#Select activation-function(hidden layers) pass below respective string to select any Activation Fuction Eg:\"tanh\" in ActivationFunction\n",
    "# \"sigmoid\": SigmoidFunction, \"softmax\": SoftmaxFunction, \"tanh\": TanhFunction, \"relu\":ReLUFunction\n",
    "act_fun_hidden = ActivationFunction(\"tanh\")\n",
    "\n",
    "#Select activation - function for output layer\n",
    "act_fun_output = ActivationFunction(\"softmax\")\n",
    "model_2 = FFNN(optimizer, layers,  loss_fun , act_fun_hidden, act_fun_output, 32, 15, initialization = \"Xavier\")\n",
    "model_2.fit(x_mnist_train, y_OH_train_mnist, x_mnist_val, y_OH_val_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(\"nadam\")\n",
    "optimizer.set_initial_parameters(parameters_nadam)\n",
    "\n",
    "\n",
    "#  \"cross_entropy\" : Cross Entropy Loss Function,  \"squared_loss\" : Squared Error Loss Function\n",
    "loss_fun = LossFunction(\"cross_entropy\")\n",
    "\n",
    "#Select activation-function(hidden layers) pass below respective string to select any Activation Fuction Eg:\"tanh\" in ActivationFunction\n",
    "# \"sigmoid\": SigmoidFunction, \"softmax\": SoftmaxFunction, \"tanh\": TanhFunction, \"relu\":ReLUFunction\n",
    "act_fun_hidden = ActivationFunction(\"relu\")\n",
    "\n",
    "#Select activation - function for output layer\n",
    "act_fun_output = ActivationFunction(\"softmax\")\n",
    "model_3 = FFNN(optimizer, layers,  loss_fun , act_fun_hidden, act_fun_output, 64, 16, initialization = \"Xavier\")\n",
    "model_3.fit(x_mnist_train, y_OH_train_mnist, x_mnist_val, y_OH_val_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model_1.predict(x_mnist_test)\n",
    "y_pred2 = model_2.predict(x_mnist_test)\n",
    "y_pred3 = model_3.predict(x_mnist_test)\n",
    "\n",
    "\n",
    "print(\"Model-1 Test Accuracy:-\", accuracy_score(np.argmax(y_OH_test_mnist,1), np.argmax(y_pred1,1)))\n",
    "print(\"Model-2 Test Accuracy:-\", accuracy_score(np.argmax(y_OH_test_mnist,1), np.argmax(y_pred2,1)))\n",
    "print(\"Model-3 Test Accuracy:-\", accuracy_score(np.argmax(y_OH_test_mnist,1), np.argmax(y_pred3,1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
